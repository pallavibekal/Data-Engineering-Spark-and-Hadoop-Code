{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "M5- 4_PySpark_Transform_and_Actions_C.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pallavibekal/Data-Engineering-Spark-and-Hadoop-Code/blob/main/M5_4_PySpark_Transform_and_Actions_C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3oZtbys8jYL"
      },
      "source": [
        "## Information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nR2ZLd-8jYM"
      },
      "source": [
        "**Overview about Spark, PySpark and Apache Spark in simple language**\n",
        "\n",
        "**Spark:** A data computational framework that handles Big data.\n",
        "\n",
        "**PySpark:** A tool to support Python with Spark\t\n",
        "\n",
        "**Apache Spark:** It is an open-source cluster-computing framework, built around speed, ease of use, and streaming analytics.\n",
        "\n",
        "* Like Spark, PySpark helps data scientists to work with (RDDs) Resilient Distributed Datasets. It is also used to work on Data frames. PySpark can be used to work with machine learning algorithms as well.\n",
        "\n",
        "### ***Spark RDD is a major concept in Apache Spark***\n",
        "\n",
        "**Resilient Distributed Datasets:**\n",
        "\n",
        "**Resilient:**    because RDDs are immutable (can’t be modified once created)                        and fault tolerant.\n",
        "\n",
        "**Distributed:**  because it is distributed across clusters\n",
        "\n",
        "**Dataset:**      because it holds data.\n",
        "\n",
        "**Why RDD?**\n",
        "\n",
        "* Apache Spark lets you treat your input files almost like any other variable, which you cannot do in Hadoop MapReduce. \n",
        "* RDDs are automatically distributed across the network by means of Partitions.\n",
        "\n",
        "RDDs are divided into smaller chunks called Partitions, and when you execute some action, a task is launched per partition. This means, the more the number of partitions, the more will be the parallelism. \n",
        "\n",
        "Spark automatically decides the number of partitions that an RDD has to be divided into, but you can also specify the number of partitions when creating an RDD. These partitions of an RDD are distributed across all the nodes in the network.\n",
        "\n",
        "**Difference between Dataframe and RDD (Resilient Distributed Datasets):**\n",
        "\n",
        "**Dataframe:**\n",
        "* Automatically finds out the schema of the dataset.\n",
        "* Performs aggregation faster than RDDs, as it provides an easy API to perform aggregation operations.\n",
        "\n",
        "**RDD:**\n",
        "* We need to define the schema manually.\n",
        "* RDD is slower than Dataframes to perform simple operations like grouping the data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-PfJvpLuTFa"
      },
      "source": [
        "**Creating an RDD**\n",
        "\n",
        "**There are three ways to create an RDD in Spark:**\n",
        "1. Parallelizing already existing collection in the driver program.\n",
        " \n",
        "  The key point to note in a parallelized collection is the number of partitions the dataset is divided into. Spark will run one task for each partition of the cluster. We require two to four partitions for each CPU in the cluster. Spark sets the number of partition based on our cluster. \n",
        "\n",
        "2. Referencing a dataset in an external storage system (e.g. HDFS, Hbase, shared file system).\n",
        "  \n",
        "  In Spark, the distributed dataset can be formed from any data source supported by Hadoop, including the local file system, HDFS, Cassandra, HBase etc. In this, the data is loaded from the external dataset.\n",
        "\n",
        "  * csv (String path): It loads a CSV file and returns the result as a Dataset.\n",
        "\n",
        "  * json (String path): It loads a JSON file (one object per line) and returns the result as a Dataset\n",
        "\n",
        "  * textFile (String path) It loads text files and returns a Dataset of String.\n",
        "\n",
        "3. Creating RDD from already existing RDDs.\n",
        "\n",
        "  Transformation mutates one RDD into another RDD, this transformation is the way to create an RDD from an already existing RDD. This creates a difference between Apache Spark and Hadoop MapReduce. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0zutSoUD0cs"
      },
      "source": [
        "**Actions/Transformations**\n",
        "\n",
        "There are two types of operations that you can perform on an RDD- \n",
        "* Transformations \n",
        "* Actions. \n",
        "\n",
        "**Transformation** applies some function on an RDD and creates a new RDD, it does not modify the RDD that you apply the function on. Also, the new RDD keeps a pointer to its parent RDD.\n",
        "\n",
        "When you call a transformation, Spark does not execute it immediately, instead it creates a lineage. A lineage keeps track of what all transformations have to be applied on that RDD, including from where it has to read the data.\n",
        "\n",
        "\n",
        "**Action** is used to either save the result to some location or to display it. You can also print the RDD lineage information by using the command: \n",
        "\n",
        "\"filtered.toDebugString\" -> (*filtered* is the RDD here)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ja7Mz2TMhIR"
      },
      "source": [
        "![img](https://cdn.iisc.talentsprint.com/CDS/Images/Pyspark_RDD.JPG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNLA8HiKxQhc"
      },
      "source": [
        "### Setup Steps:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZfXZMpmdJ8p"
      },
      "source": [
        "#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n",
        "Id = \"2200092\" #@param {type:\"string\"}"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HM6RuGGdJ8-"
      },
      "source": [
        "#@title Please enter your password (your registered phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
        "password = \"9686800288\" #@param {type:\"string\"}"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBPPuGmBlDIN",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5c82f9e7-c0ce-488d-d704-ffde8caf2dde"
      },
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "from IPython import get_ipython\n",
        "\n",
        "ipython = get_ipython()\n",
        "  \n",
        "notebook= \"M5_AST_04_PySpark_Transform_and_Actions_C\" #name of the notebook\n",
        "\n",
        "def setup():\n",
        "#  ipython.magic(\"sx pip3 install torch\")  \n",
        "    ipython.magic(\"sx wget https://cdn.iisc.talentsprint.com/CDS/Datasets/Spark_Text.txt\")\n",
        "    ipython.magic(\"sx wget https://cdn.iisc.talentsprint.com/CDS/Datasets/google_books.csv\")\n",
        "    from IPython.display import HTML, display\n",
        "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n",
        "    print(\"Setup completed successfully\")\n",
        "    return\n",
        "\n",
        "def submit_notebook():\n",
        "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
        "    \n",
        "    import requests, json, base64, datetime\n",
        "\n",
        "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
        "    if not submission_id:\n",
        "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "\n",
        "      if r[\"status\"] == \"Success\":\n",
        "          return r[\"record_id\"]\n",
        "      elif \"err\" in r:        \n",
        "        print(r[\"err\"])\n",
        "        return None        \n",
        "      else:\n",
        "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
        "        return None\n",
        "    \n",
        "    elif getAnswer() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n",
        "      f = open(notebook + \".ipynb\", \"rb\")\n",
        "      file_hash = base64.b64encode(f.read())\n",
        "\n",
        "      data = {\"complexity\" : Complexity, \"additional\" :Additional, \n",
        "              \"concepts\" : Concepts, \"record_id\" : submission_id, \n",
        "              \"answer\" : Answer, \"id\" : Id, \"file_hash\" : file_hash,\n",
        "              \"notebook\" : notebook,\n",
        "              \"feedback_experiments_input\" : Comments,\n",
        "              \"feedback_mentor_support\": Mentor_support}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "      if \"err\" in r:        \n",
        "        print(r[\"err\"])\n",
        "        return None   \n",
        "      else:\n",
        "        print(\"Your submission is successful.\")\n",
        "        print(\"Ref Id:\", submission_id)\n",
        "        print(\"Date of submission: \", r[\"date\"])\n",
        "        print(\"Time of submission: \", r[\"time\"])\n",
        "        print(\"View your submissions: https://cds.iisc.talentsprint.com/notebook_submissions\")\n",
        "        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
        "        return submission_id\n",
        "    else: submission_id\n",
        "    \n",
        "\n",
        "def getAdditional():\n",
        "  try:\n",
        "    if not Additional: \n",
        "      raise NameError\n",
        "    else:\n",
        "      return Additional  \n",
        "  except NameError:\n",
        "    print (\"Please answer Additional Question\")\n",
        "    return None\n",
        "\n",
        "def getComplexity():\n",
        "  try:\n",
        "    if not Complexity:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Complexity\n",
        "  except NameError:\n",
        "    print (\"Please answer Complexity Question\")\n",
        "    return None\n",
        "  \n",
        "def getConcepts():\n",
        "  try:\n",
        "    if not Concepts:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Concepts\n",
        "  except NameError:\n",
        "    print (\"Please answer Concepts Question\")\n",
        "    return None\n",
        "  \n",
        "  \n",
        "# def getWalkthrough():\n",
        "#   try:\n",
        "#     if not Walkthrough:\n",
        "#       raise NameError\n",
        "#     else:\n",
        "#       return Walkthrough\n",
        "#   except NameError:\n",
        "#     print (\"Please answer Walkthrough Question\")\n",
        "#     return None\n",
        "  \n",
        "def getComments():\n",
        "  try:\n",
        "    if not Comments:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Comments\n",
        "  except NameError:\n",
        "    print (\"Please answer Comments Question\")\n",
        "    return None\n",
        "  \n",
        "\n",
        "def getMentorSupport():\n",
        "  try:\n",
        "    if not Mentor_support:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Mentor_support\n",
        "  except NameError:\n",
        "    print (\"Please answer Mentor support Question\")\n",
        "    return None\n",
        "\n",
        "def getAnswer():\n",
        "  try:\n",
        "    if not Answer:\n",
        "      raise NameError \n",
        "    else: \n",
        "      return Answer\n",
        "  except NameError:\n",
        "    print (\"Please answer Question\")\n",
        "    return None\n",
        "  \n",
        "\n",
        "def getId():\n",
        "  try: \n",
        "    return Id if Id else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "def getPassword():\n",
        "  try:\n",
        "    return password if password else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "submission_id = None\n",
        "### Setup \n",
        "if getPassword() and getId():\n",
        "  submission_id = submit_notebook()\n",
        "  if submission_id:\n",
        "    setup() \n",
        "else:\n",
        "  print (\"Please complete Id and Password cells before running setup\")\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId=2200092&recordId=6436\"></script>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup completed successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Q9PCT0kBVB3"
      },
      "source": [
        " **Install PySpark**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Xzay908G5qQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4434afd-25fe-4773-e236-12118094eedf"
      },
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.2.0.tar.gz (281.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.3 MB 39 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.2\n",
            "  Downloading py4j-0.10.9.2-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 40.5 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.0-py2.py3-none-any.whl size=281805911 sha256=3cc51fd5279016259e53aae9608afc26ec0528aa1e3bbbc20180fc0635cbaa08\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/de/d2/9be5d59d7331c6c2a7c1b6d1a4f463ce107332b1ecd4e80718\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.2 pyspark-3.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BobYAePRT6Ok"
      },
      "source": [
        "**Creating Spark Session**\n",
        "\n",
        "Spark session is a combined entry point of a Spark application, which came into implementation from Spark 2.0 (Instead of having various contexts, everything is encapsulated in a Spark session)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-tj32cQHmBb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "f3dabf4b-0c2b-41af-f832-11ed568c91d7"
      },
      "source": [
        "# Start spark session\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, udf  # User Defined Functions\n",
        "from pyspark.sql.types import StringType\n",
        "spark = SparkSession.builder.appName('Rdd').getOrCreate()\n",
        "spark"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://88fce200dc3e:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.2.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Rdd</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f84d8d648d0>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypfIQnQczPMy"
      },
      "source": [
        "# Accessing sparkContext from sparkSession instance.\n",
        "sc = spark.sparkContext"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Kt4YX3C3dGT"
      },
      "source": [
        "### Spark Python Transformations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuBu_zaiYWX-"
      },
      "source": [
        "**map()** - A map transformation is useful when we need to transform an RDD by applying a function to each element. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vJJS0NS_o8y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0b07ae4-523a-417d-e675-9cbfc93dafc1"
      },
      "source": [
        "# Return a new RDD by applying a function to each element of this RDD.\n",
        "rdd = sc.parallelize([\"b\", \"a\", \"c\"])\n",
        "sorted(rdd.map(lambda x: (x, 1)).collect())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('a', 1), ('b', 1), ('c', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRmlweKeDvjf"
      },
      "source": [
        "**take()** - Take the first num elements of the RDD.\n",
        "\n",
        "It works by first scanning one partition, and use the results from that partition to estimate the number of additional partitions needed to satisfy the limit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGskOAX2_s0x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ca70491-6480-4cf6-e484-d6f9e0986be3"
      },
      "source": [
        "sc.parallelize([2, 3, 4, 5, 6]).cache().take(2) #take()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 3]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oZr8ukWkHAW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba7d66a4-dbe9-4e3e-d684-4545990ec40f"
      },
      "source": [
        "sc.parallelize(range(100), 100).filter(lambda x: x > 90).take(3) #take()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[91, 92, 93]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3fEehJeEguG"
      },
      "source": [
        "**flatMap()** - The flatMap transformation will return a new RDD by first applying a function to all elements of this RDD, and then flattening the results. This is the main difference between the flatMap and *map transformations.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAelIgUTf3Sy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "423b0f84-fa6d-45ad-b8af-1b16a86bbfb2"
      },
      "source": [
        "s0 = sc.parallelize([3,4,5])\n",
        "s0.flatMap(lambda x: [x, x*x]).collect()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3, 9, 4, 16, 5, 25]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJLRChBQxnmG"
      },
      "source": [
        "Compare the same function using map()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBJegE2cxmAq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae7f011e-f54e-4991-9a86-a3641f29a332"
      },
      "source": [
        "sc.parallelize([3,4,5]).map(lambda x: [x,  x*x]).collect()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[3, 9], [4, 16], [5, 25]]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WihTRM55aQzK"
      },
      "source": [
        "**filter()** - The filter transformation returns a new dataset formed by selecting  those elements of the source on which func returns true."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgypZMSQGyR_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9d34ac6-731a-4bb6-930f-fe1f2b6a583c"
      },
      "source": [
        "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
        "rdd.filter(lambda x: x % 2 == 0).collect() # Return a new RDD containing only the elements that satisfy a predicate."
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 4]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FV3bYIKJi31"
      },
      "source": [
        "**groupByKey()** - We can apply the “groupByKey” transformations on (key,val) pair RDD. The “groupByKey” will group the values for each key in the original RDD. It will create a new pair, where the original key corresponds to this collected group of values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyh3Z6mDOMbM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e28647b-fdd2-449f-d8c1-4c1e7d522361"
      },
      "source": [
        "x = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
        "x.groupByKey().map(lambda x : (x[0], list(x[1]))).collect() "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('b', [1]), ('a', [1, 1])]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjbYWjBCLfUn"
      },
      "source": [
        "**reduceByKey()** - Merge the values for each key using an associative reduce function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_kSwLv6OmzV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a52a113-f433-474d-83bc-985907ef8a91"
      },
      "source": [
        "from operator import add\n",
        "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
        "sorted(rdd.reduceByKey(add).collect())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('a', 2), ('b', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UV-_pmJVLfl5"
      },
      "source": [
        "**mapPartitions()** - Is similar to map, but runs separately on each partition (block) of the RDD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2UtmrsS5uA0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a84e77a2-174c-4f38-b67c-1b90dc39e43a"
      },
      "source": [
        "wordsList = ['cat', 'elephant', 'rat', 'rat', 'cat']\n",
        "\n",
        "wordsRDD = sc.parallelize(wordsList, 4) # number of partitions - 4\n",
        "\n",
        "print(wordsRDD.collect())\n",
        "\n",
        "itemsRDD = wordsRDD.mapPartitions(lambda iterator: [','.join(iterator)])\n",
        "# mapPartitions() loops through 4 partitions and combines('rat,cat') in 4th iteration.\n",
        "print (itemsRDD.collect())"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['cat', 'elephant', 'rat', 'rat', 'cat']\n",
            "['cat', 'elephant', 'rat', 'rat,cat']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBZnXOHiHMjU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc8b8cd1-1760-43a9-f46d-022900daa3c9"
      },
      "source": [
        "L = range(1,10)\n",
        "\n",
        "parallel = sc.parallelize(L, 3) # number of partitions - 3\n",
        "\n",
        "def f(iterator): \n",
        "  yield sum(iterator)\n",
        "\n",
        "parallel.mapPartitions(f).collect()\n",
        "\n",
        "# Results [6,15,24] are created because mapPartitions() loops through 3 partitions, Partion 1: 1+2+3 = 6, Partition 2: 4+5+6 = 15, Partition 3: 7+8+9 = 24\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[6, 15, 24]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rclESZfdRbMM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0ceeb5c-dd0a-47ea-9f11-1e185b01a64d"
      },
      "source": [
        "rdd = sc.parallelize([1, 2, 3, 4], 2) # number of partitions - 2\n",
        "\n",
        "def f(iterator):\n",
        "  yield sum(iterator)\n",
        "\n",
        "rdd.mapPartitions(f).collect() \n",
        "\n",
        "# Results [3, 7], partition 1 : 1+2 = 3, partition 2 : 3+4 =7"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3, 7]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-SUV2VcJjMJ"
      },
      "source": [
        "**mapPartitionsWithIndex()** - Return a new RDD by applying a function to each partition of this RDD, while tracking the index of the original partition."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dgV6f21Us_m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a27b1430-0621-46ce-ae2a-1b57376733f9"
      },
      "source": [
        "rdd = sc.parallelize([1, 2, 3, 4], 4)\n",
        "def f(splitIndex, iterator): yield splitIndex\n",
        "rdd.mapPartitionsWithIndex(f).sum()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82fgwxs8kzy5"
      },
      "source": [
        "### Spark Python Actions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaYgbjhklcT0"
      },
      "source": [
        "**Creating an RDD to explain \"RDD actions with Examples\"**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seJ35vrGleKN"
      },
      "source": [
        "data=[(\"Z\", 1),(\"A\", 20),(\"B\", 30),(\"C\", 40),(\"B\", 30),(\"B\", 60)]\n",
        "\n",
        "inputRDD = spark.sparkContext.parallelize(data)\n",
        "  \n",
        "listRdd = spark.sparkContext.parallelize([1,2,3,4,5,3,2])\n",
        "\n",
        "from operator import add"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTRyVfftnLaD"
      },
      "source": [
        "After creating two RDDs as given above, we use these two as and when necessary to demonstrate the RDD actions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bjcr2K9fk0LT"
      },
      "source": [
        "**first()** – Return the first element in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kR-hsPBqnds7"
      },
      "source": [
        "#first\n",
        "print(\"first :  \"+str(listRdd.first()))\n",
        "print(\"first :  \"+str(inputRDD.first()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFIF21pnk0tc"
      },
      "source": [
        "**take()** – Return the first num elements of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8iyLrZ0nopK"
      },
      "source": [
        "#take()\n",
        "print(\"take : \"+str(listRdd.take(2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5dG25M-k02C"
      },
      "source": [
        "**takeSample()** – Return the subset of the dataset in an Array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J58_ENwkn15A"
      },
      "source": [
        "print(\"take : \"+str(listRdd.takeSample(0,3))) # ([1,2,3,4,5,3,2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XddOZ8LRk08Y"
      },
      "source": [
        "**takeOrdered()** – Return the first num (smallest) elements from the dataset and this is the opposite of the take() action. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nptmLS_qoSKt"
      },
      "source": [
        "print(\"takeOrdered : \"+ str(listRdd.takeOrdered(2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-chsPGWPk1EQ"
      },
      "source": [
        "**collect()** - Return the complete dataset as an Array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMDnTnA0pNHG"
      },
      "source": [
        "#Collect\n",
        "data = listRdd.collect()\n",
        "print(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1How8QYk1NK"
      },
      "source": [
        "**count()** – Return the count of elements in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5YNtF_RpW6y"
      },
      "source": [
        "print(\"Count : \"+str(listRdd.count()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNSjv_LCpcAU"
      },
      "source": [
        "**countByValue()** – Return Map[T,Long] key representing each unique value in dataset and value represents count each value present."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ij3GMKfbpcLs"
      },
      "source": [
        "print(\"countByValue :  \"+str(listRdd.countByValue()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNHqoneIplsk"
      },
      "source": [
        "**reduce()** – Reduces the elements of the dataset using the specified binary operator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McRegvjapl0T"
      },
      "source": [
        "redRes=listRdd.reduce(add)\n",
        "print(redRes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yklH98ELqV_q"
      },
      "source": [
        "**top()** – Return top n elements from the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pj1f2tsCqWWj"
      },
      "source": [
        "print(\"top : \"+str(listRdd.top(2)))\n",
        "print(\"top : \"+str(inputRDD.top(2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OONqU-CTtEkZ"
      },
      "source": [
        "**fold()** - Aggregate the elements of each partition, and then the results for all the partitions, using a given associative function and a neutral \"zero value.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euxrIYOYtFEu"
      },
      "source": [
        "foldRes=listRdd.fold(0, add)\n",
        "print(foldRes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khV8HhGc0lhx"
      },
      "source": [
        "**foldByKey()** -  is quite similar to fold(), both use a zero value of the same type of the data in our RDD and combination function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aGQdKuM0L9n"
      },
      "source": [
        "inputRDD.foldByKey(0, add).collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehIGu2bG0-kc"
      },
      "source": [
        "**reduceByKey()** - Merge the values for each key using an associative reduce function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PXKBSyi0-zi"
      },
      "source": [
        "sorted(inputRDD.reduceByKey(add).collect())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6S5yGzv13Ck"
      },
      "source": [
        "**combineByKey()** - Generic function to combine the elements for each key using a custom set of aggregation functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSpVQHCL13Ls"
      },
      "source": [
        "def f(inputRDD):\n",
        "  return inputRDD\n",
        "def add(A, B):\n",
        "  return A + str(B)\n",
        "sorted(inputRDD.combineByKey(str, add, add).collect())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFXAbayxXKtw"
      },
      "source": [
        "### PySpark User Defined Functions "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXEXD6LUXFu7"
      },
      "source": [
        "* PySpark UDF is a User Defined Function that is used to create a reusable \n",
        "function in Spark.\n",
        "\n",
        "* Once UDF is created, that can be re-used on multiple DataFrames and SQL (after registering).\n",
        "\n",
        "* The default type of the udf() is StringType."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCyKMMvCGRkk"
      },
      "source": [
        "Created dataframe with two columns \"Seqno\" and \"Name\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvCmOPMTDtZv"
      },
      "source": [
        "columns = [\"Seqno\",\"Name\"]\n",
        "data = [(\"1\", \"john jones\"),\n",
        "    (\"2\", \"tracey smith\"),\n",
        "    (\"3\", \"amy sanders\")]\n",
        "\n",
        "df = spark.createDataFrame(data=data,schema=columns)\n",
        "\n",
        "df.show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lon2qElWGW8Z"
      },
      "source": [
        "Applying UDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9TQBD5JZTh3"
      },
      "source": [
        "# creating a udf using lambda\n",
        "convertUDF = udf(lambda z: z.upper())\n",
        "df.select(col(\"Seqno\"), convertUDF(col(\"Name\")).alias(\"Name\") ).show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiEIyKJ4xmKj"
      },
      "source": [
        "#### **Shuffle Operations**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XP9I8tYbl_i"
      },
      "source": [
        "Shuffling is a mechanism PySpark uses to redistribute the data across different executors and even across machines. PySpark shuffling triggers when we perform certain transformation operations like gropByKey(), reduceByKey(), join() on RDDS\n",
        "\n",
        "Spark also supports transformations with wide dependencies, such as groupByKey and reduceByKey. In these dependencies, the data required to compute the records in a single partition can reside in many partitions of the parent dataset.\n",
        "\n",
        "To perform these transformations, all of the tuples with the same key must end up in the same partition, processed by the same task. To satisfy this requirement, Spark performs a shuffle, which transfers data around the cluster and results in a new stage with a new set of partitions. \n",
        "\n",
        "For example, consider the following code:\n",
        "\n",
        "**sc.textFile(\"someFile.txt\").map(mapFunc).flatMap(flatMapFunc).filter(filterFunc).count()**\n",
        "\n",
        "It runs a single action, count, which depends on a sequence of three transformations on a dataset derived from a text file. This code runs in a single stage because none of the outputs of these three transformations depend on data that comes from different partitions than their inputs. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huYvlG3IAq_A"
      },
      "source": [
        "**Below is an example implementing RDD based model to count the words given in a file**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-b3HdfYQjLe-"
      },
      "source": [
        "\n",
        "\n",
        "To implement RDD based model, we have used the text file (**Spark_Text.txt**) which includes Apache Spark notes/information. This text file contains 5 paragraphs of information on Spark.\n",
        "\n",
        "We would perform RDD Transformations and Actions on the file to count the words given in the text file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1WbnApmkBle"
      },
      "source": [
        "rdd = sc.textFile(\"Spark_Text.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gpTlIzzYH3V"
      },
      "source": [
        "# To lower the case of each word of a document, we can use the map transformation.\n",
        "def Func(lines):\n",
        "      lines = lines.lower()\n",
        "      lines = lines.split()\n",
        "      return lines\n",
        "rdd1 = rdd.map(Func)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-h8E_vJiplH"
      },
      "source": [
        "rdd1.take(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3zknNLjYH-d"
      },
      "source": [
        "#To get the flat output, we need to apply a transformation which will flatten the output, The transformation “flatMap\" will help here:\n",
        "rdd2 = rdd.flatMap(Func)\n",
        "rdd2.take(3)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0wfA1urYIB0"
      },
      "source": [
        "rdd3 = rdd2.filter(lambda x:x!= '')\n",
        "rdd3.take(7)  # We can check first 7 elements of “rdd3” by applying take action."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0c9lxazfYIFM"
      },
      "source": [
        "rdd3_mapped = rdd3.map(lambda x: (x,1))\n",
        "rdd3_grouped = rdd3_mapped.groupByKey()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIozl8hjYV0P"
      },
      "source": [
        "rdd3_mapped.reduceByKey(lambda x,y: x+y).map(lambda x:(x[1],x[0])).sortByKey(False).take(200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTbilp60aHcl"
      },
      "source": [
        "**In the below example we can see Spark Transformations in Python using a CSV file.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqdCmWOejijF"
      },
      "source": [
        "We will use this CSV file (**Google_Books.csv**) to work on Spark Transformations.\n",
        "\n",
        "This data was acquired from the Google Books store. Google API was used to acquire the data. Nine features were gathered for each book in the data set. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P357wQsFb5XS"
      },
      "source": [
        "book_names = sc.textFile(\"google_books.csv\")\n",
        "rows = book_names.map(lambda line: line.split(\",\")) #we are creating a new RDD called “rows” by splitting every row in the book_names RDD.  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0wNy70CdRpC"
      },
      "source": [
        "for row in rows.take(rows.count()):\n",
        "  print(row[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n01Rlq6wee8X"
      },
      "source": [
        "for row in rows.take(10):\n",
        "  print(row[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvdfQzEXjDq1"
      },
      "source": [
        "# filter() - Creating a new RDD by returning only the elements that satisfy the search filter.\n",
        "rows.filter(lambda line: \"Inward Journey\" in line).collect() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKguXKc1lAtF"
      },
      "source": [
        "# groupByKey() The following groups all titles to their publisher. Operates on value pairs\n",
        "rows = book_names.map(lambda line: line.split(\",\"))\n",
        "titleToPublisher = rows.map(lambda n: (str(n[0]),str(n[6]) )).groupByKey()\n",
        "titleToPublisher.map(lambda x : {x[0]: list(x[1])}).take(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHfHdGCP_n6Y"
      },
      "source": [
        "### Please answer the questions below to complete the experiment:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgSwVENIPcM6"
      },
      "source": [
        "# @title Select the False Statement: { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Answer = \"flatMap function always produces a single value as output for each input value\" #@param [\"\", \"Map transforms an RDD of length N into another RDD of length N\",\"flatMap function always produces a single value as output for each input value\",\"The reduce operation shuffles and reduces the output obtained from the map\"]"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMzKSbLIgFzQ"
      },
      "source": [
        "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Complexity = \"Good and Challenging for me\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjcH1VWSFI2l"
      },
      "source": [
        "#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
        "Additional = \"na\" #@param {type:\"string\"}\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VBk_4VTAxCM"
      },
      "source": [
        "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Concepts = \"Yes\" #@param [\"\",\"Yes\", \"No\"]\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XH91cL1JWH7m"
      },
      "source": [
        "#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Comments = \"Very Useful\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8xLqj7VWIKW"
      },
      "source": [
        "#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Mentor_support = \"Very Useful\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzAZHt1zw-Y-",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9e6c0fc-65ab-4aaa-93b1-812cb65b7579"
      },
      "source": [
        "#@title Run this cell to submit your notebook for grading { vertical-output: true }\n",
        "try:\n",
        "  if submission_id:\n",
        "      return_id = submit_notebook()\n",
        "      if return_id : submission_id = return_id\n",
        "  else:\n",
        "      print(\"Please complete the setup first.\")\n",
        "except NameError:\n",
        "  print (\"Please complete the setup first.\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your submission is successful.\n",
            "Ref Id: 6436\n",
            "Date of submission:  23 Jan 2022\n",
            "Time of submission:  12:04:51\n",
            "View your submissions: https://cds.iisc.talentsprint.com/notebook_submissions\n"
          ]
        }
      ]
    }
  ]
}